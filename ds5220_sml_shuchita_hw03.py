# -*- coding: utf-8 -*-
"""DS5220 SML Shuchita HW03.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1K8lH-V-psqnEFa5dLWSBF9Ms6ubiOiEl

#**DS5220 SML HW03 Programming Solution**

Importing libraries
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error as MSE
from sklearn.metrics import accuracy_score, classification_report
import matplotlib.pyplot as plt
import itertools
import math
from math import *
import random

"""**1. Download HW-2-2 from previous homework.**"""

import scipy.io
mat=scipy.io.loadmat('/content/drive/MyDrive/Colab Notebooks/data_2_2b.mat')

X_train_p2_2 = mat['X_trn']
Y_train_p2 = mat['Y_trn']
X_test_p2_2 = mat['X_tst']
Y_test_p2 = mat['Y_tst']

"""##**Using train data, Implement splitting algorithm of decision tree based on information gain.**"""

data = pd.DataFrame({'x1':X_train_p2_2[:,0], 
                    'x2':X_train_p2_2[:,1],
                    'y' :Y_train_p2[:,0]})

data.head(5)

"""**Sort feature values and extend it to response variable.**"""

#Sorting by x1 and splitting data
x1_data = data.sort_values(by = ['x1'])
x1_data = x1_data.drop(columns="x2")
x1_data.head(5)

"""Functions to calculate entropy, information_gain"""

#Given a Pandas dataframe, it calculates the entropy
def calc_entropy(data):
  val_0 = len(data[data["y"]==0])
  val_1 = len(data[data["y"]==1])
  if val_0 == 0:
    entropy = 0
  elif val_1 == 0 :
    entropy = 0
  else :
    a_0 = val_0/len(data)
    a_1 = val_1/len(data)
    b_0 = a_0*np.log2(a_0)
    b_1 = a_1*np.log2(a_1)
    #since we know this has only 2 features we manually take the sum
    entropy = -(b_0 + b_1)
  return entropy


# Implementing splitting algorithm of decision tree based on information gain.
# Consider 20 feature values hence in range(20) 
def get_decisiontree(data):
  initial_entropy = calc_entropy(data)
  val = math.floor(len(data)/20)
  p = np.zeros([20,2])
  # Initializing index list
  index_list = []
  # Initializing weighted entropy list
  weighted_ent = []
  # Current number of groups 
  # Here I have used floor value in order to generate spilt index. 
  # Since 136/20 = 6.8, instead of rounding it to 7 I have used floor value 6.
  curr = 6
  for i in range(20):
    index = curr
    data_1 = data[0:index]
    data_2 = data[index:]
    d_1 = len(data_1)/len(data)
    d_2 = len(data_2)/len(data)
    weighted_entropy = d_1*calc_entropy(data_1)+d_2*calc_entropy(data_2)
    index_list.append(str(i))
    weighted_ent.append(str(weighted_entropy))
    information_gain = initial_entropy - weighted_entropy
    p[i,0] = information_gain
    p[i,1] = index
    curr = curr+val
  weighted_entropy_for_all_indices = pd.DataFrame(list(zip(index_list, weighted_ent)), columns = ['Index', 'Weighted Entropy'])
  weighted_entropy_for_all_indices = weighted_entropy_for_all_indices.set_index('Index')
  return p

"""**Consider 20 feature values such that their indices in training data are at equal distance from each other, and calculate information gain. For example, if data length is 100, take value at index 0, 10, 20, ....,100.**"""

#Calculating the entropy for x1
x1_ig = get_decisiontree(x1_data)
x1_ig_data = pd.DataFrame(x1_ig, columns=["Information Gain","Index"])
x1_ig = pd.DataFrame(x1_ig)
x1_ig_data['Index'] = x1_ig_data['Index'].astype(int)
x1_ig_data = x1_ig_data.set_index('Index')
print("\n")
print("Considering 20 feature values such that their indices in x1 data are at equal distance from each other, the Information gain is :")
print(x1_ig_data)
index_x1 = x1_ig[x1_ig[0]==x1_ig[0].max()]
index_x1 = index_x1.reset_index(drop=True)
print("The maximum value of information gain for feature x1 is {} and the corresponding index is {} ".format(index_x1[0][0],int(index_x1[1][0])))

#Sorting by x2 and splitting data
x2_data = data.sort_values(by = ['x2'])
x2_data = x2_data.drop(columns="x1")
x2_data.head(5)

x2_ig = get_decisiontree(x2_data)
x2_ig_data = pd.DataFrame(x2_ig, columns=["Information Gain","Index"])
x2_ig = pd.DataFrame(x2_ig)
x2_ig_data['Index'] = x2_ig_data['Index'].astype(int)
x2_ig_data = x2_ig_data.set_index('Index')
print("\n")
print("Considering 20 feature values such that their indices in x2 data are at equal distance from each other, the Information gain is :")
print(x2_ig_data)
index_x2 = x2_ig[x2_ig[0]==x2_ig[0].max()]
index_x2 = index_x2.reset_index(drop=True)
print("The maximum value of information gain for feature x2 is {} and the corresponding index is {} ".format(index_x2[0][0],int(index_x2[1][0])))

""" **Q1 A**"""

#Recording feature value with the best information gain.
max_info = [[index_x1[0][0],0,index_x2[1][0]],[index_x2[0][0],1,index_x2[1][0]]]
max_info = pd.DataFrame(max_info)
max_infogain = max_info[max_info[0]==max_info[0].max()]
max_infogain = max_infogain.reset_index(drop=True)
print("The maximum value of information gain is {} for the feature x{} at split index {}.".format(max_infogain[0][0],max_infogain[1][0]+1,int(max_infogain[2][0])))

"""**Thus, the best feature value is at index = 66.0 for x1 for which the information gain is the highest at 0.9036592795505546**

**Q1 B**

**In training data, change labels from 0 to 1 or 1 to 0 for 10 random samples**
"""

# Choosing 10 random samples using the sample function
# Generate random indices from the data to swap y values of
random_index = set()
size = 10
answerSize = 0
random.seed(3)
while answerSize < 10:
    r = random.randint(0,len(data))
    if r not in random_index:
      answerSize += 1
      random_index.add(r)
random_index = list(random_index)
print(random_index)

# Creating a new data copy to store new changed data
data_elements_vice_versa = data.copy()
print(data_elements_vice_versa)

# Swapping y values in new data 
for i in range(len(random_index)):
# Case 1 : Swapping y values in new data from 0 to 1 and vice versa
  data_elements_vice_versa.iloc[random_index[i],2] = 1 - data_elements_vice_versa.iloc[random_index[i],2]
# Case 2 : Swapping y values in new data from 0 to 1 only
# if data_elements_0_only.iloc[random_index[i],2] == 0:
#    data_elements_0_only.iloc[random_index[i],2] = 1
# Case 3 : Swapping y values in new data from 1 to 0 only
#  if data_elements_1_only.iloc[random_index[i],2] == 1:
#    data_elements_1_only.iloc[random_index[i],2] = 0  
# print(data.iloc[random_index[i],2], "has now become ", data_elements_1_only.iloc[random_index[i],2])

print("\n==========\n", data_elements_vice_versa)

"""Repeat step 1, and report feature, best split value and information gain."""

#Sorting by x1 and splitting the new data with 10 random samples
x1_new_data_vv = data_elements_vice_versa.sort_values(by = ['x1'])
x1_new_data_vv = x1_new_data_vv.drop(columns="x2")
#print(x1_new_data_vv)

# Where we changed 0s to 1 AND 1s to 0
x1_new_vv_ig = get_decisiontree(x1_new_data_vv)
x1_new_vv_ig_data = pd.DataFrame(x1_new_vv_ig, columns=["Information Gain","Index"])
x1_new_vv_ig = pd.DataFrame(x1_new_vv_ig)
x1_new_vv_ig_data['Index'] = x1_new_vv_ig_data['Index'].astype(int)
x1_new_vv_ig_data = x1_new_vv_ig_data.set_index('Index')
print("\n")
print("Considering 20 feature values such that their indices in x1 data are at equal distance from each other, the Information gain is :")
print(x1_new_vv_ig_data)
index_new_x1_vv = x1_new_vv_ig[x1_new_vv_ig[0]==x1_new_vv_ig[0].max()]
index_new_x1_vv = index_new_x1_vv.reset_index(drop=True)
print("The maximum value of information gain for feature x1 in the new sampled data is {} and the corresponding index is {} ".format(index_new_x1_vv[0][0],int(index_new_x1_vv[1][0])))

#Sorting by x2 and splitting the new data with 10 random samples
x2_new_data_vv = data_elements_vice_versa.sort_values(by = ['x2'])
x2_new_data_vv = x2_new_data_vv.drop(columns="x1")
#print(x2_new_data_vv)

# Where we changed 0s to 1 AND 1s to 0
x2_new_vv_ig = get_decisiontree(x2_new_data_vv)
x2_new_vv_ig_data = pd.DataFrame(x2_new_vv_ig, columns=["Information Gain","Index"])
x2_new_vv_ig = pd.DataFrame(x2_new_vv_ig)
x2_new_vv_ig_data['Index'] = x2_new_vv_ig_data['Index'].astype(int)
x2_new_vv_ig_data = x2_new_vv_ig_data.set_index('Index')
print("\n")
print("Considering 20 feature values such that their indices in x1 data are at equal distance from each other, the Information gain is :")
print(x2_new_vv_ig_data)
index_new_x2_vv = x2_new_vv_ig[x2_new_vv_ig[0]==x2_new_vv_ig[0].max()]
index_new_x2_vv = index_new_x2_vv.reset_index(drop=True)
print("The maximum value of information gain for feature x2 in the new sampled data is {} and the corresponding index is {} ".format(index_new_x2_vv[0][0],int(index_new_x2_vv[1][0])))

"""**Record feature with best information in this new sampled data**"""

max_info_vv = [[index_new_x1_vv[0][0],0,index_new_x1_vv[1][0]],[index_new_x2_vv[0][0],1,index_new_x2_vv[1][0]]]
max_info_vv = pd.DataFrame(max_info_vv)
max_infogain_vv = max_info_vv[max_info_vv[0]==max_info_vv[0].max()]
max_infogain_vv = max_infogain_vv.reset_index(drop=True)
print("The maximum value of information gain is {} for the feature x{} at split index {}.".format(max_infogain_vv[0][0],max_infogain_vv[1][0]+1,int(max_infogain_vv[2][0])))

"""**Q1 C**

Explain the differences between (a) and (b).
"""

summary_data = pd.DataFrame()
summary_data['Instance'] = ['Original', 'Swap vice versa']
summary_data['Feature : X_'] = [max_infogain[1][0], max_infogain_vv[1][0]]
summary_data['Maximum_Information_Gain'] = [max_infogain[0][0], max_infogain_vv[0][0]]
summary_data['Splitting_Index'] = [int(max_infogain[2][0]), int(max_infogain_vv[2][0])]

summary_data

"""It can be observed that **when we swap 0 with 1 and 1 with 0** : The information gain reduces but the splitting index remains same. 

Hence, it can be concluded that the information gain varies drastically when we make small changes to a small sample of the dataset. This suggests that decision tree is overfitting (as is it's tendency to) and that it has a **high variance** which may result in changing the entire tree structure.
Hence, we use bagging to tackle the problem caused by high variance in decision trees. Since we are dealing with Decision Trees, and we have inferred we need to **add bagging** to decrease variance, we welcome the idea of **Random Forests** which is a bagging algorithm useful in such dynamic cases. 
Thus, using Random Forests would be wise in such a dynamic case.

### References: 

1.   https://arxiv.org/pdf/1405.2061.pdf
2.   Brownlee, J. (2019, October 13). A Gentle Introduction to Information Entropy. Machine Learning Mastery. https://machinelearningmastery.com/what-is-information-entropy/
3. How to code decision tree in Python from scratch. (2021, January 19). Ander Fernández. https://anderfernandez.com/en/blog/code-decision-tree-python-from-scratch/
4. Decision Tree Split Methods | Decision Tree Machine Learning. (2020, June 29). Analytics Vidhya. https://www.analyticsvidhya.com/blog/2020/06/4-ways-split-decision-tree/#h2_6
"""